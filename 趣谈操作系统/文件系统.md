## 1 文件系统

### 1.1 文件系统的功能规划

最常用的外部存储就是硬盘，数据以文件的形式保存在硬盘上，为了管理这些文件，在规划文件系统的时候需要考虑以下几点：

- 文件系统有**严格的组织形式**，使得文件能够以块为单位存储
- 文件系统也要有**索引区**，用来方便查找一个文件分成的多个块都存放在了什么位置
- 如果有热点文件，需要有**缓存**
- 文件应该使用**文件夹的形式组织起来**，方便管理查询
- Linux内核需要在内存里面**维护一套数据结构**，保存哪些文件被哪些进程打开和使用

### 1.2 文件系统相关命令行

**格式化**

- 将一块盘使用命令组织称一定格式的文件系统的过程
- Windows常用格式化格式为**NTFS**，Linux常用的是**ext3或者ext4**
- 当一个 Linux 系统插入了一块没有格式化的硬盘的时候，我们可以通过命令 `fdisk -l`，查看格式化和没有格式化的分区
- 可以通过命令 `mkfs.ext3` 或者 `mkfs.ext4` 进行格式化
- **格式化后的硬盘需要挂载到某个目录下才能作为普通的文件系统访问**

### 1.3 文件系统相关系统调用

**1、打开文件**

- 当使用系统调用open打开一个文件时，操作系统会创建一些数据结构来表示这个被打开的文件。在进程中，为这个打开的文件分配一个**文件描述符fd（File Descriptor）**

**2、文件描述符fd（File Descriptor）**

- 区分一个进程打开的多个文件
- **只在当前进程有效**
- open返回的fd必须记录好，我们对这个文件的所有操作都需要这个fd

**3、写入文件**

- **参数**
  - 文件描述符（你是谁？）
  - 表示写入的数据存放位置（你在哪里？）
  - 表示希望写入的字节数
- 返回值：成功写入文件的字节数量

<img src="https://img-blog.csdnimg.cn/696aa6549fc6425a84ac3e4d5d9096ef.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 33%;" />

## 2 硬盘文件系统

> 主要讨论Linux下主流的文件系统格式——ext系列文件系统格式

### 2.1 inode与块的存储

1、硬盘被分成相同大小的单元，称为**块（block）**，一块的大小是扇区大小的整数倍（默认4k）

2、**inode**

- 存放文件的**元数据**，例如名字、权限等
- **每个文件都对应一个inode**，文件夹也是文件

- 数据结构：

  ```c
  struct ext4_inode {
    __le16  i_mode;    /* 读写权限 */
    __le16  i_uid;    /* 属于哪个用户 */
    __le16  i_gid;    /* 属于哪个组 */
    __le32  i_size_lo;  /* 大小 */
    
    __le32  i_atime;  /* 最近一次访问文件的时间 */
    __le32  i_ctime;  /* 最近一次更改inode的时间 */
    __le32  i_mtime;  /* 最近一次修改文件的时间，只有文件数据修改才会更新 */
    __le32  i_dtime;  /* Deletion Time */
    
    __le16  i_links_count;  /* Links count */
    __le32  i_blocks_lo;  /* 占多少个块 */
    __le32  i_flags;  /* File flags */
  ......
    __le32  i_block[EXT4_N_BLOCKS];/* 指向block */
    __le32  i_generation;  /* File version (for NFS) */
    __le32  i_file_acl_lo;  /* File ACL */
    __le32  i_size_high;
  ......
  };
  ```

**3、block是如何保存的？**

- EXT4_N_BLOCKS有如下定义：

  - 前12项保存块的位置，通过`i_block[0-11]`可以获取保存文件内容的块
  - 如果一个文件放不下的时候，就需要让`i_block[12]`指向一个**间接块**，存放数据块的位置
  - 如果文件再大一些，`i_block[13]`会指向一个块，可以用**二次间接块**，存放**间接块**的位置；再大就会使用`i_block[14]`指向**三次间接块**

  ```c
  #define  EXT4_NDIR_BLOCKS    12
  #define  EXT4_IND_BLOCK      EXT4_NDIR_BLOCKS
  #define  EXT4_DIND_BLOCK      (EXT4_IND_BLOCK + 1)
  #define  EXT4_TIND_BLOCK      (EXT4_DIND_BLOCK + 1)
  #define  EXT4_N_BLOCKS      (EXT4_TIND_BLOCK + 1)
  ```

  <img src="https://img-blog.csdnimg.cn/b5f0edf0b8824bb5ae2c67532cc15db8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:33%;" />
  - **问题**：对于大文件需要多次读取硬盘才能找到块，访问速度较慢

    - **解决**：ext4引入了**Extents**

      - 存放连续的块，保存为一颗树

      - 节点头**ext4_extent_header**：描述某个节点

        ```c
        struct ext4_extent_header {
          __le16  eh_magic; 
          __le16  eh_entries;  /* 表示这个节点有多少项，项分为两种，如果是叶子节点就直接指向硬盘上的连续块地址（ext4_extent），如果是分支节点就会指向下一层的节点（ext4_extent_idx） */
          __le16  eh_max;    
          __le16  eh_depth;  
          __le32  eh_generation; 
        };
        ```

      - inode里面的i_block可以放下一个1个header和4个extent；这个时候eh_depth位0

      - 如果文件比较大，4个extent放不下就需要分裂成一棵树，eh_depth>0的节点就是索引节点最底层eh_depth=0的是叶子节点

      ![在这里插入图片描述](https://img-blog.csdnimg.cn/973e13c51d39429aa9dd4f67387bf3e4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70)

**4、inode位图和块位图**

- 文件系统中，专门有一个块来保存inode的位图，在这4k里面每一位对应一个inode。如果是1表示这个inode已经被使用了，如果是0就是没有被使用

### 2.2 文件系统的格式

**1、块组**

- 数据块的位图是放在一个块里面的，共4k。每位代表一个块，共可以表示 4∗1024∗8=215 个数据块。如果每个数据块也是按默认的 4K，最大可以表示空间为 215∗4∗1024=227 个 byte，也就是 128M。
- 如果采用**“一个块的位图 + 一系列的块”**，外加**“一个块的 inode 的位图 + 一系列的 inode 的结构”**，最多能够表示 128M，称为一个**块组**

2、**块组描述符表**

- 这样一个个块组，就基本构成了我们整个文件系统的结构。因为块组有多个，块组描述符也同样组成一个列表，我们把这些称为**块组描述符表**。

3、**超级块**（ext4_super_block）

- **对整个文件系统的情况进行描述**
- 这里面有整个文件系统一共有多少 inode，s_inodes_count；一共有多少块，s_blocks_count_lo，每个块组有多少 inode，s_inodes_per_group，每个块组有多少块，s_blocks_per_group 等。这些都是这类的全局信息。

**4、文件系统的格式**

- 引导块：系统启动时预留的一块区域
- 默认情况下，超级块和块组描述符表都有副本保存在每一个块组里面

![在这里插入图片描述](https://img-blog.csdnimg.cn/bc3342f2aabb409995f255507a582835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70)

- **问题**：如果每个块组都保存一份完整的块组描述符，浪费空间，且块组描述符的个数就决定了整个文件系统的大小，就被限制住了

**5、Meta Block Groups**

- 块组描述符表不保存所有块组的描述符，而是将块组分为多个**元块组（Meta Block Group）**，每个元块组里面的块描述符表仅包括自己的，一个元块组包含64个块组

<img src="https://img-blog.csdnimg.cn/ad7e65b77eed4ffbbb58ad7a591f2ddf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

### 2.3 目录的存储格式

**1、目录和普通文件**

- 相同：本身也是一个文件，也有inode，inode里面也指向一些块。

- 不同：普通文件的块保存文件数据，目录文件的块保存目录里面一项一项的**文件信息**
- 文件信息：保存这个目录下一级文件的文件名和对应的inode，通过inode找到真正的文件

2、为避免频繁读取磁盘里的目录文件，内核会把已经读过的目录文件用`目录项`这个数据结构缓存在内存，方便用户下次读取目录信息，目录项可包含目录或文件，不要惊讶于可以保存目录，目录格式的目录项里面保存的是目录里面一项一项的文件信息。

### 2.4 软链接和硬连接

<img src="https://img-blog.csdnimg.cn/119c6e8ac78243efbf93a4fad2441742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

**硬链接**：老文件A被创建若干个硬链接B、C后。A、B、C三个文件的inode是相同的，所以不能跨文件系统。同时只有ABC全部删除，系统才会删除源文件。

**软链接**：相当于基于老文件A新建了个文件B，该文件B有新的inode，不过文件B内容是老文件A的路径。所以软链接可以跨文件系统。当老文件A删除后，文件B仍然存在，不过找不到指定文件了。

### 2.5 总结

<img src="https://img-blog.csdnimg.cn/bc9ae2f2498c4af6972703c89bdee346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

## 3 虚拟文件系统

**多层合作**

- 应用层：进程可通过**系统调用**进行文件读写操作
- 在内核，每个进程都需要为打开的文件维护一定的数据结构
- 在内核，整个系统打开的文件也需要维护一定的数据结构
- **虚拟文件系统**：Linux可支持多种文件系统，他们的实现各不相同，因此Linux内核向用户空间提供了虚拟文件系统这个统一的接口对文件系统进行操作。它提供了常见的文件系统对象模型，例如 inode、directory entry、mount 等，以及操作这些对象的方法，例如 inode operations、directory operations、file operations 等
- **真正的文件系统**：例如ext4
- 为了加快设备读写效率，需要缓存层
- 为了读写文件系统，需要通过块设备I/O层，这是文件系统层和块设备驱动的接口

<img src="https://img-blog.csdnimg.cn/c39245e3789a4014b40c3c9de468a6d3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1NjUwODk5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:40%;" />

### 3.1 open 打开/创建文件

1、在进程里面通过open系统调用打开文件，最终调用系统调用实现`sys_open`

```c
SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)
{
......
  return do_sys_open(AT_FDCWD, filename, flags, mode);
}


long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)
{
......
  fd = get_unused_fd_flags(flags);
  if (fd >= 0) {
    struct file *f = do_filp_open(dfd, tmp, &op);
    if (IS_ERR(f)) {
      put_unused_fd(fd);
      fd = PTR_ERR(f);
    } else {
      fsnotify_open(f);
      fd_install(fd, f);
    }
  }
  putname(tmp);
  return fd;
}
```

**2、首先获取一个没有用的文件描述符**

- 在每个进程的task_struct中，有一个指针files，类型是files_struct

```c
struct files_struct    *files;
```

- `files_struct`里面最重要的是一个**文件描述符表**，每打开一个文件，会在这个列表分配一项，下标就是**文件描述符**

```c
struct files_struct {
  ...... 
    struct file __rcu * fd_array[NR_OPEN_DEFAULT];
};
```

- 对于任何一个进程，默认情况下，文件描述符0表示`stdin`标准输入，文件描述符1表示`stdout`标准输出，文件描述符2表示`stedeer`标准错误输出；再打开的文件会从这个列表找一个空闲位置分配给它（**不是递增的**）
- 文件描述符表每一项都是指向**struct file**的指针，也就是说每打开一个文件都会有一个struct file对应

3、`do_filp_open`创建`struct file`结构

4、`fd_install`将文件描述符和这个结构关联起来

## 4 Linux I/O读写方式

Linux 提供了 3 种磁盘与主存之间的数据传输机制：

- **轮询**：基于死循环对 I/O 端口进行不断检测
- **I/O 中断**：当数据到达时，磁盘主动向 CPU 发起中断请求，由 CPU 自身负责数据的传输过程
- **DMA 传输**：在 I/O 中断的基础上引入了 DMA 磁盘控制器，由 DMA 磁盘控制器负责数据的传输，降低了 I/O 中断操作对 CPU 资源的大量消耗

> DMA（Direct Memory Access，直接存储器访问)）
>
> - 是所有现代电脑的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于CPU的大量中断负载。否则，CPU 需要从来源把每一片段的资料复制到暂存器，然后把它们再次写回到新的地方。在这个时间中，CPU 对于其他的工作来说就无法使用。



### 4.1 I/O中断

每次用户进程读取磁盘数据时，都需要 CPU 中断，然后发起 I/O 请求等待数据读取和拷贝完成，**每次的 I/O 中断都导致 CPU 的上下文切换。**

- 用户进程向 CPU 发起 `read `系统调用读取数据，由用户态切换为内核态，然后一直阻塞等待数据的返回
- CPU 在接收到指令以后对磁盘发起 I/O 请求，将磁盘数据先放入磁盘控制器缓冲区
- 数据准备完成以后，磁盘向 CPU 发起 I/O 中断
- CPU 收到 I/O 中断以后将磁盘缓冲区中的数据拷贝到内核缓冲区，然后再从内核缓冲区拷贝到用户缓冲区
- 用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时间钟

<img src="https://img-blog.csdnimg.cn/a1d6db90dd62469ebe70afee4c98403c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:40%;" />

### 4.2 DMA传输

1、DMA （Direct Memory Access），是一种**允许外围设备（硬件子系统）直接访问系统主内存的机制**。也就是说，基于 DMA 访问方式，**系统主内存于硬盘或网卡之间的数据传输可以绕开 CPU 的全程调度**。目前大多数的硬件设备，包括磁盘控制器、网卡、显卡以及声卡等都支持 DMA 技术。

**2、整个数据传输操作在一个 DMA 控制器的控制下进行的**。CPU 除了在数据传输开始和结束时做一点处理外（开始和结束时候要做中断处理），在传输过程中 CPU 可以继续进行其他的工作。这样**在大部分时间里，CPU 计算和 I/O 操作都处于并行操作**，使整个计算机系统的效率大大提高。

- 用户进程向 CPU 发起 `read `系统调用读取数据，由用户态切换为内核态，然后一直阻塞等待数据的返回
- CPU 在接收到指令以后对 **DMA 磁盘控制器**发起调度指令（**CPU第一次操作**）
- DMA 磁盘控制器对磁盘发起 I/O 请求，将磁盘数据先放入磁盘控制器缓冲区，CPU 全程不参与此过程。
- 数据读取完成后，DMA 磁盘控制器会接受到磁盘的通知，将数据**从磁盘控制器缓冲区拷贝到内核缓冲区**。
- DMA 磁盘控制器向 CPU 发出数据读完的信号，由 **CPU 负责将数据从内核缓冲区拷贝到用户缓冲区**（**CPU第二次操作**）
- 用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时间钟

<img src="https://img-blog.csdnimg.cn/3cb54ea82f414751a9f74e46beeb49a0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:40%;" />



### 4.3 传统I/O方式

1、在 Linux 系统中，传统的访问方式是通过` write() `和` read() `两个系统调用实现的，通过` read() `函数读取文件到到缓存区中，然后通过` write() `方法把缓存中的数据输出到网络端口

2、传统 I/O 操作的数据读写流程，整个过程涉及 **2 次 CPU 拷贝、2 次 DMA 拷贝总共 4 次拷贝，以及 4 次上下文切换**

<img src="https://img-blog.csdnimg.cn/96886b4f07624009af9770d74f8d9d52.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:50%;" />

**3、传统读操作**

- 当应用程序执行` read `系统调用读取一块数据的时候，如果这块数据已经存在于用户进程的页内存中，就直接从内存中读取数据；如果数据不存在，则先将数据从磁盘加载数据到内核空间的读缓存（read buffer）中，再从读缓存拷贝到用户进程的页内存中。

```
read(file_fd, tmp_buf, len);
```

- 基于传统的 I/O 读取方式，`read `系统调用会触发 2 次上下文切换，1 次 DMA 拷贝和 1 次 CPU 拷贝，发起数据读取的流程如下：
  - 用户进程通过 `read() `函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）
  - CPU利用DMA控制器将数据**从主存或硬盘拷贝到内核空间的读缓冲区**
  - CPU**将读缓冲区中的数据拷贝到用户空间的用户缓冲区**
  - 上下文从内核态（kernel space）切换回用户态（user space），read 调用执行返回

**4、传统写操作**

- 当应用程序准备好数据，执行 `write `系统调用发送网络数据时，先将数据从用户空间的页缓存拷贝到内核空间的网络缓冲区（socket buffer）中，然后再将写缓存中的数据拷贝到网卡设备完成数据发送。

```
write(socket_fd, tmp_buf, len);
```

- 基于传统的 I/O 写入方式，`write() `系统调用会触发 2 次上下文切换，1 次 CPU 拷贝和 1 次 DMA 拷贝，用户程序发送网络数据的流程如下：
  - 用户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）
  - CPU **将用户缓冲区（user buffer）中的数据拷贝到内核空间（kernel space）的网络缓冲区（socket buffer）**
  - CPU 利用 DMA 控制器**将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输**
  - 上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回

### 4.4 零拷贝

在 Linux 中零拷贝技术主要有 3 个实现思路：**用户态直接 I/O、减少数据拷贝次数以及写时复制技术**

- **用户态直接 I/O**：应用程序可以**直接访问硬件存储，操作系统内核只是辅助数据传输**。这种方式依旧存在用户空间和内核空间的上下文切换，**硬件上的数据直接拷贝至了用户空间**，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。
- **减少数据拷贝次数**：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝，这也是**当前主流零拷贝技术的实现思路**。
- **写时复制技术**：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。

#### 4.4.1 用户态直接I/O

1、用户态直接 I/O 使得应用进程或运行在用户态（user space）下的库函数**直接访问硬件设备**，数据直接跨过内核进行传输，内核在数据传输过程除了进行必要的虚拟存储配置工作之外，不参与任何其他工作，这种方式能够直接**绕过内核**，极大提高了性能。

<img src="https://img-blog.csdnimg.cn/4dac9f8a56174038b8f060d3eaa62aab.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:50%;" />

**2、使用场景**：用户态直接 I/O 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。

**3、缺点**：这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费

- 解决方案：配合异步 I/O 使用。

#### 4.4.2 减少数据拷贝次数：mmap + write

**1、使用 mmap + write 代替原来的 read + write 方式，减少了 1 次 CPU 拷贝操作**

**2、`mmap`方法：**

- mmap 是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址，mmap + write 的伪代码如下：

```c
tmp_buf = mmap(file_fd, len);
write(socket_fd, tmp_buf, len);
```

**3、使用 mmap 的目的**

- 将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射，从而实现内核缓冲区与应用程序内存的共享，**省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程**，**然而内核读缓冲区（read buffer）仍需将数据拷贝到内核写缓冲区（socket buffer）**

**4、过程**

- 基于 `mmap + write `系统调用的零拷贝方式，整个拷贝过程会发生 **4 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝**，用户程序读写数据的流程如下：

  - 用户进程通过 `mmap() `函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
  - **将用户进程的内核空间的读缓冲区（read buffer）与用户空间的缓存区（user buffer）进行内存地址映射**。
  - CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
  - 上下文从内核态（kernel space）切换回用户态（user space），mmap 系统调用执行返回。
  - 用户进程通过` write() `函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
  - **CPU将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）**。
  - CPU利用DMA控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
  - 上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。

  <img src="https://img-blog.csdnimg.cn/43ccf2d3ece44f91a20cb5c31b127d9d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:50%;" />

**5、mmap 主要的用处是提高 I/O 性能**，特别是针对大文件。**对于小文件，内存映射文件反而会导致碎片空间的浪费**，因为内存映射总是要对齐页边界，最小单位是 4 KB，一个 5 KB 的文件将会映射占用 8 KB 内存，也就会浪费 3 KB 内存。

**6、mmap 的拷贝虽然减少了 1 次拷贝，提升了效率，但也存在一些隐藏的问题**

- 当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么 write 系统调用会因为访问非法地址被 SIGBUS 信号终止，SIGBUS 默认会杀死进程并产生一个 coredump，服务器可能因此被终止。

#### 4.4.3 减少数据拷贝次数：sendfile

1、目的是**简化通过网络在两个通道之间进行的数据传输过程**。`sendfile `系统调用的引入，**不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数**，它的伪代码如下：

```c
sendfile(socket_fd, file_fd, len);
```

2、通过 sendfile 系统调用，**数据可以直接在内核空间内部进行 I/O 传输**，从而省去了数据在用户空间和内核空间之间的来回拷贝。

3、与 mmap 内存映射方式不同的是， **`sendfile` 调用中 I/O 数据对用户空间是完全不可见的**。也就是说，这是一次完全意义上的数据传输过程。

**4、过程**

基于 sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 **2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝**，用户程序读写数据的流程如下：

- 用户进程通过` sendfile() `函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
- CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
- **CPU 将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）**。
- **CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输**。
- 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。

<img src="https://img-blog.csdnimg.cn/3fe876c9e7bc4f64890bfbee6a9f84e7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:50%;" />



5、相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是仍然有 1 次 CPU 拷贝操作。**sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。**



#### 4.4.4 减少数据拷贝次数：sendfile + DMA gather copy

1、Linux 2.4 版本的内核对 `sendfile `系统调用进行修改，为  DMA 拷贝引入了 `gather` 操作。

- 它将内核空间（kernel space）的读缓冲区（read buffer）中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区（ socket  buffer）中，**由 DMA 根据内存地址、地址偏移量将数据批量地从读缓冲区（read buffer）拷贝到网卡设备中**
- 这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作，sendfile 的伪代码如下：

```C
sendfile(socket_fd, file_fd, len);
```

2、在硬件的支持下，sendfile 拷贝方式不再从内核缓冲区的数据拷贝到 socket 缓冲区，取而代之的**仅仅是缓冲区文件描述符和数据长度的拷贝**，这样 DMA 引擎直接利用 gather 操作将页缓存中数据打包发送到网络中即可，本质就是和虚拟内存映射的思路类似。

**3、过程**

基于 sendfile + DMA gather copy 系统调用的零拷贝方式，整个拷贝过程会发生 **2 次上下文切换、0 次 CPU 拷贝以及 2 次 DMA 拷贝**，用户程序读写数据的流程如下：

- 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
- CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
- **CPU 把读缓冲区（read buffer）的文件描述符（file descriptor）和数据长度拷贝到网络缓冲区（socket buffer）**。
- 基于已拷贝的文件描述符（file descriptor）和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量地将数据从内核的读缓冲区（read buffer）拷贝到网卡进行数据传输。
- 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。

<img src="https://img-blog.csdnimg.cn/8b729ae8db464c85985653ea84d587ff.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:50%;" />

4、sendfile + DMA gather copy 拷贝方式同样存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，**它只适用于将数据从文件拷贝到 socket 套接字上的传输过程。**



#### 4.4.5 减少数据拷贝次数：splice

1、sendfile 只适用于将数据从文件拷贝到 socket 套接字上，同时需要硬件的支持，这也限定了它的使用范围。Linux 在 2.6.17 版本引入 splice 系统调用，不仅不需要硬件支持，还实现了两个文件描述符之间的数据零拷贝。splice 的伪代码如下：

```c
splice(fd_in, off_in, fd_out, off_out, len, flags);
```

2、splice 系统调用可以在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline），从而避免了两者之间的 CPU 拷贝操作。

**3、过程**

基于 splice 系统调用的零拷贝方式，整个拷贝过程会发生 **2 次上下文切换，0 次 CPU 拷贝以及 2 次 DMA 拷贝**，用户程序读写数据的流程如下：

- 用户进程通过 splice() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
- CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
- CPU 在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline）。
- CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
- 上下文从内核态（kernel space）切换回用户态（user space），splice 系统调用执行返回。

<img src="/Users/zhangtao/Desktop/截屏2021-09-13 下午11.08.18.png" alt="截屏2021-09-13 下午11.08.18" style="zoom:50%;" />

4、splice 拷贝方式也同样存在**用户程序不能对数据进行修改的问题**。除此之外，它使用了 Linux 的管道缓冲机制，可以用于任意两个文件描述符中传输数据，但是它的两个文件描述符参数中有一个必须是管道设备。

#### 4.4.6 总结

无论是传统 I/O 拷贝方式还是引入零拷贝的方式，**2 次 DMA Copy 是都少不了的**，因为两次 DMA 都是依赖硬件完成的。

下面从 CPU 拷贝次数、DMA 拷贝次数以及系统调用几个方面总结一下上述几种 I/O 拷贝方式的差别

- **传统（read+write）**
  - 发起read
  - **DMA拷贝**：拷贝到cpu内核空间的读缓冲区
  - **cpu拷贝**：内核空间读缓冲区拷贝到用户空间用户缓冲区
  - 发起write
  - **cpu拷贝**：用户缓冲区拷贝到网络缓冲区
  - **DMA拷贝**：DMA将数据从网络缓冲区拷贝到网卡
- **内存映射（mmap+write）**
  - 发起mmap
  - 用户缓冲区和读缓冲区进行内存地址映射
  - **DMA拷贝**：DMA将数据拷贝到读缓冲区
  - 发起write
  - **cpu拷贝**：读缓冲区拷贝到网络缓冲区
  - **DMA拷贝**：DMA将数据从网络缓冲区拷贝到网卡
- **sendfile**
  - 发起sendfile
  - **DMA拷贝**：将数据拷贝到读缓冲区
  - **cpu拷贝**：数据拷贝到网络缓冲区
  - **DMA拷贝**：DMA将数据从网络缓冲区拷贝到网卡
- **sendfile+DMA gather copy**
  - 发起sendfile
  - **DMA拷贝**：将数据拷贝到读缓冲区
  - CPU 把读缓冲区（read buffer）的文件描述符（file descriptor）和数据长度拷贝到网络缓冲区（socket buffer）
  - **DMA拷贝**：基于已拷贝的文件描述符和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量地将数据从内核的读缓冲区（read buffer）拷贝到网卡进行数据传输
- **splice**
  - 发起splice
  - **DMA拷贝**：将数据拷贝到读缓冲区
  - CPU 在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline）
  - **DMA拷贝**：将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输

<img src="https://img-blog.csdnimg.cn/05e7459b25f7439497b2c0eaea154891.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5b-r5LmQ55qE5Yay5rWq56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:50%;" />
